{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from underthesea import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cấu hình tùy chọn cho Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thiết bị sử dụng: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Thiết bị sử dụng: {device}\")\n",
    "\n",
    "def remove_emoji(text):\n",
    "    for emoji in UNICODE_EMOJI.values():\n",
    "        text = text.replace(emoji, \"\")\n",
    "    for emoticon in EMOTICONS_EMO.values():\n",
    "        text = text.replace(emoticon, \"\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', ' <num> ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r\"[!@#$\\[\\]()']\", \"\", text)\n",
    "\n",
    "    # Load stopwords once and use set for faster lookup\n",
    "    with open('vietnamese-stopwords.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = set(f.read().split(\"\\n\"))\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    text = \" \".join(word for word in words if word not in stopwords)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentence, tokenizer, max_length=128):\n",
    "    sentence = remove_emoji(sentence)\n",
    "    sentence = clean_text(sentence)\n",
    "    tokens = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    return tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, max_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last output of the LSTM\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path, device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Mô hình đã được nạp từ {path}\")\n",
    "\n",
    "# Hàm kiểm tra mô hình\n",
    "def test_model(model, tokenizer, sentence, device):\n",
    "    model.eval()\n",
    "    input_ids = preprocess_input(sentence, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        _, predicted_label = torch.max(outputs, 1)\n",
    "    label_map = {0: \"Tích cực (POS)\", 1: \"Trung tính (NEU)\", 2: \"Tiêu cực (NEG)\"}\n",
    "    return label_map[predicted_label.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_size = 64\n",
    "output_size = 3\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(vocab_size, embedding_dim, hidden_size, output_size, max_length).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình đã được nạp từ ./model/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_28852\\3551613524.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"./model/best_lstm_model.pth\"\n",
    "load_model(model, best_model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kết quả thử nghiệm:\n",
      "Câu: \"Tớ thất vọng về cậu quá\" --> Dự đoán: Tiêu cực (NEG)\n",
      "Câu: \"cái này thật là tệ hại!\" --> Dự đoán: Tiêu cực (NEG)\n",
      "Câu: \"Sản phẩm này tốt\" --> Dự đoán: Tích cực (POS)\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán với các câu đầu vào\n",
    "test_sentences = [\n",
    "    '''Tớ thất vọng về cậu quá''',\n",
    "    '''cái này thật là tệ hại!''',\n",
    "    '''Sản phẩm này tốt'''\n",
    "]\n",
    "\n",
    "print(\"\\nKết quả thử nghiệm:\")\n",
    "for sentence in test_sentences:\n",
    "    prediction = test_model(model, tokenizer, sentence, device)\n",
    "    print(f\"Câu: \\\"{sentence}\\\" --> Dự đoán: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
